{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03 (part B): Working with ML algorithms in Scikit-learn\n",
    "By Dr Ivan Olier-Caparroso, 30/01/22\n",
    "\n",
    "\n",
    "## Task\n",
    "We will perform a two-class classification task on the *South African Heart* dataset (SAHeart). The dataset is publicly available, just type its name in Google. It is also available in the module GitHub's data repository (https://raw.githubusercontent.com/iaolier/7021DATSCI/main/data/SAheart.csv). The dataset is a retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa. There are roughly two controls per case of Coronary Heart Disease (CHD). Many of the CHD positive men have undergone blood pressure reduction treatment and other programs to reduce their risk factors after their CHD event. In some cases the measurements were made after these treatments. These data are taken from a larger dataset, described in Rousseauw et al, 1983, South African Medical Journal. \n",
    "\n",
    "This is the set of variables in the dataset:\n",
    "\n",
    "* sbp - systolic blood pressure\n",
    "* tobacco - cumulative tobacco (kg)\n",
    "* ldl - low densiity lipoprotein cholesterol\n",
    "* adiposity\n",
    "* famhist - family history of heart disease (Present, Absent)\n",
    "* typea - type-A behavior\n",
    "* obesity\n",
    "* alcohol - current alcohol consumption\n",
    "* age - age at onset\n",
    "* chd - response, coronary heart disease\n",
    "\n",
    "The aim is to predict the risk of CHD as a function of the other variables. This is essentially a classification task with two classes: CHD/No CHD (coded as 1 and 0, respectively).\n",
    "\n",
    "## Activities\n",
    "\n",
    "1. Clean the data (if needed) and convert binary, any categorical variable. \n",
    "2. Split the data into training and test subsets, following out-of-sample resampling strategy. Leave 30% for testing.\n",
    "3. Standardise the data splits (remove mean and divide by standard deviation).\n",
    "\n",
    "-- Default score for the model evaluation is the AUC.\n",
    "\n",
    "4. Perform hyperparameter tuning on logistic regression (LR) and select the best possible model. *Scikit-learn* has several LR implementations (or solvers), that implements the maximum-likelihood algorithm. we commonly work with the *saga* solver, which allows for the *elastic net* penalisation, which is a generalisation of the *ridge* and *lasso* penalisations we studied. Ridge pensalisation uses the $\\mathcal{l}_{1}$-norm, whilst lasso, the $\\mathcal{l}_{2}$-norm. Elastic net allows for any norm between $\\mathcal{l}_{1}$ and $\\mathcal{l}_{2}$. In addition, the solver uses a complexity parameter `C` in a similar fashion as *SVM*. More details can be found here: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression. Use several `C` and `l1_ratio`.\n",
    "5. Perform hyperparameter tuning on k-nearest neighbour (KNN) and select the best possible model. Use several `k` values.\n",
    "6. Perform hyperparameter tuning on support vector machines (SVM) and select the best possible model. Use several `C` values, and several kernel functions (and kernel hyperparameters).\n",
    "7. Report test AUCs of optimised models and indicate the best one. Comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAHeart = pd.read_csv('https://raw.githubusercontent.com/iaolier/7021DATSCI/main/data/SAheart.csv')\n",
    "SAHeart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAHeart.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(SAHeart, alpha=0.2, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAHeart.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(SAHeart[\"tobacco\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(SAHeart[\"alcohol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(SAHeart[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(SAHeart[['famhist']])\n",
    "SAHeart = SAHeart.drop('famhist', axis=1)\n",
    "SAHeart = pd.concat([SAHeart, dummies[['famhist_Present']]], axis=1)\n",
    "SAHeart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(SAHeart, alpha=0.2, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample resampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(SAHeart.drop('chd', axis = 1), SAHeart.chd, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further split the training set, so we have a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lr = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "lr_val_probs = m_lr.predict_proba(X_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_val_probs = lr_val_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "lr_trn_probs = m_lr.predict_proba(X_train)\n",
    "lr_trn_probs = lr_trn_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic model: AUC: Training = %.3f, Validation = %.3f' % (roc_auc_score(y_train, lr_trn_probs),\n",
    "                                                                   roc_auc_score(y_val, lr_val_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svm = SVC(probability=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "svm_val_probs = m_svm.predict_proba(X_val)\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_val_probs = svm_val_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "svm_trn_probs = m_svm.predict_proba(X_train)\n",
    "svm_trn_probs = svm_trn_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM model: AUC: Training = %.3f, Validation = %.3f' % (roc_auc_score(y_train, svm_trn_probs),\n",
    "                                                              roc_auc_score(y_val, svm_val_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C' : [0.1, 1.0, 10, 100], 'l1_ratio' : [0, 0.25, 0.5, 0.75, 1]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(LogisticRegression(penalty='elasticnet', solver='saga', random_state=1, max_iter=10000),\n",
    "                  param_grid,\n",
    "                  scoring = 'roc_auc')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid scores on validation set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C' : [0.1, 1.0, 10, 100], 'kernel' : ['rbf'], 'gamma' : [0.0001, 0.001, 0.01, 0.1, 1]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(probability=True, random_state=1),\n",
    "                  param_grid,\n",
    "                  scoring = 'roc_auc')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid scores on validation set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Use a random search instead of a grid search to find the optimal hyperparameters. Compare with the above results. Any comments?\n",
    "\n",
    "### Exercise 2:\n",
    "Implemment an artificial neural network on the SAHeart data. Tune several ANN hyperparameters such as learning rate, number of neurons per hidden layer and number of hidden layers. Is there any ANN that performs better than the above implementations?\n",
    "\n",
    "### Exercise 3:\n",
    "From the above model performances, identify any model that might be overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
